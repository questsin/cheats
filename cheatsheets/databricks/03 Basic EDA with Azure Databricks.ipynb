{"cells":[{"cell_type":"markdown","source":["#Basic EDA with Azure Databricks"],"metadata":{}},{"cell_type":"markdown","source":["This notebook contains slightly more advanced topics like data cleaning, handling missing values, and correlation analysis.\n\nIn order to run this notebook you should have previously run the <a href=\"$./02 Loading data with Azure Databricks\">Loading data with Azure Databricks</a> notebook to get your data propery loaded."],"metadata":{}},{"cell_type":"markdown","source":["### Simple exploration"],"metadata":{}},{"cell_type":"markdown","source":["To work with this data programmatically, we can access the data using a Spark DataFrame. Run the following code to create a DataFrame from our table.\n\nBe sure to update the table name  \"usedcars\\_#####\" with the unique name created while running the <a href=\"$./02 Loading data with Azure Databricks\">Loading data with Azure Databricks</a> notebook."],"metadata":{}},{"cell_type":"code","source":["df = spark.sql(\"SELECT * FROM default.data1_csv\")\ndf"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: DataFrame[id: int, c1: int, c2: int, c3: int, c4: int, c5: int]</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["**`<IMPORTANT NOTE>`**"],"metadata":{}},{"cell_type":"markdown","source":["There are two major types of dataframes you will encounter in Python: Spark dataframes (sometimes referred as PySpark dataframes) and Pandas dataframes. Although they share several common features, they also differ quite a lot. Throughout the labs we will work mostly with Spark dataframes. Fortunatelly, it's very simple to convert a Spark dataframe to a Pandas dataframe. Run the next cell to get a Pandas dataframe from your Spark dataframe:"],"metadata":{}},{"cell_type":"code","source":["pdf = df.toPandas()\npdf"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>c1</th>\n      <th>c2</th>\n      <th>c3</th>\n      <th>c4</th>\n      <th>c5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>4</td>\n      <td>5</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>3</td>\n      <td>4</td>\n      <td>5</td>\n      <td>6</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>4</td>\n      <td>5</td>\n      <td>6</td>\n      <td>7</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>5</td>\n      <td>6</td>\n      <td>7</td>\n      <td>8</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>6</td>\n      <td>7</td>\n      <td>8</td>\n      <td>9</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>7</td>\n      <td>8</td>\n      <td>9</td>\n      <td>10</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>8</td>\n      <td>9</td>\n      <td>10</td>\n      <td>11</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>9</td>\n      <td>10</td>\n      <td>11</td>\n      <td>12</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>10</td>\n      <td>11</td>\n      <td>12</td>\n      <td>13</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>11</td>\n      <td>12</td>\n      <td>13</td>\n      <td>14</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>12</td>\n      <td>13</td>\n      <td>14</td>\n      <td>15</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13</td>\n      <td>13</td>\n      <td>14</td>\n      <td>15</td>\n      <td>16</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>14</td>\n      <td>14</td>\n      <td>15</td>\n      <td>16</td>\n      <td>17</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>15</td>\n      <td>15</td>\n      <td>16</td>\n      <td>17</td>\n      <td>18</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>16</td>\n      <td>16</td>\n      <td>17</td>\n      <td>18</td>\n      <td>19</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>17</td>\n      <td>17</td>\n      <td>18</td>\n      <td>19</td>\n      <td>20</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>18</td>\n      <td>18</td>\n      <td>19</td>\n      <td>20</td>\n      <td>21</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>19</td>\n      <td>19</td>\n      <td>20</td>\n      <td>21</td>\n      <td>22</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>20</td>\n      <td>20</td>\n      <td>21</td>\n      <td>22</td>\n      <td>23</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>21</td>\n      <td>21</td>\n      <td>22</td>\n      <td>23</td>\n      <td>24</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>22</td>\n      <td>22</td>\n      <td>23</td>\n      <td>24</td>\n      <td>25</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>23</td>\n      <td>23</td>\n      <td>24</td>\n      <td>25</td>\n      <td>26</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>24</td>\n      <td>24</td>\n      <td>25</td>\n      <td>26</td>\n      <td>27</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>25</td>\n      <td>25</td>\n      <td>26</td>\n      <td>27</td>\n      <td>28</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>26</td>\n      <td>26</td>\n      <td>27</td>\n      <td>28</td>\n      <td>29</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>27</td>\n      <td>27</td>\n      <td>28</td>\n      <td>29</td>\n      <td>30</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>28</td>\n      <td>28</td>\n      <td>29</td>\n      <td>30</td>\n      <td>31</td>\n      <td>32</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["Read more about these differences [here](https://databricks.com/blog/2015/08/12/from-pandas-to-apache-sparks-dataframe.html).\n\n\n**`</IMPORTANT NOTE>`**"],"metadata":{}},{"cell_type":"markdown","source":["Let's start by taking a look at our dataframe. Run the following cells to get the top 10 entries in the dataframe."],"metadata":{}},{"cell_type":"code","source":["df.head(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: [Row(id=1, c1=1, c2=2, c3=3, c4=4, c5=5),\n Row(id=2, c1=2, c2=3, c3=4, c4=5, c5=6),\n Row(id=3, c1=3, c2=4, c3=5, c4=6, c5=7),\n Row(id=4, c1=4, c2=5, c3=6, c4=7, c5=8),\n Row(id=5, c1=5, c2=6, c3=7, c4=8, c5=9),\n Row(id=6, c1=6, c2=7, c3=8, c4=9, c5=10),\n Row(id=7, c1=7, c2=8, c3=9, c4=10, c5=11),\n Row(id=8, c1=8, c2=9, c3=10, c4=11, c5=12),\n Row(id=9, c1=9, c2=10, c3=11, c4=12, c5=13),\n Row(id=10, c1=10, c2=11, c3=12, c4=13, c5=14)]</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["The next one does the same but displays the data in more organized manner."],"metadata":{}},{"cell_type":"code","source":["df.show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+---+---+---+---+\n id| c1| c2| c3| c4| c5|\n+---+---+---+---+---+---+\n  1|  1|  2|  3|  4|  5|\n  2|  2|  3|  4|  5|  6|\n  3|  3|  4|  5|  6|  7|\n  4|  4|  5|  6|  7|  8|\n  5|  5|  6|  7|  8|  9|\n  6|  6|  7|  8|  9| 10|\n  7|  7|  8|  9| 10| 11|\n  8|  8|  9| 10| 11| 12|\n  9|  9| 10| 11| 12| 13|\n 10| 10| 11| 12| 13| 14|\n+---+---+---+---+---+---+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["We can get some information about the structure of the data. Note that all columns are currently of type string (as a byproduct of the import process). Well address this issue later in this notebook."],"metadata":{}},{"cell_type":"code","source":["df.dtypes"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[6]: [(&#39;id&#39;, &#39;int&#39;),\n (&#39;c1&#39;, &#39;int&#39;),\n (&#39;c2&#39;, &#39;int&#39;),\n (&#39;c3&#39;, &#39;int&#39;),\n (&#39;c4&#39;, &#39;int&#39;),\n (&#39;c5&#39;, &#39;int&#39;)]</div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["Now let's try getting a sense of our data set by collecting some summary statistics about every column. Run the following cell."],"metadata":{}},{"cell_type":"code","source":["summary = df.describe()\ndisplay(summary)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>summary</th><th>id</th><th>c1</th><th>c2</th><th>c3</th><th>c4</th><th>c5</th></tr></thead><tbody><tr><td>count</td><td>28</td><td>28</td><td>28</td><td>28</td><td>28</td><td>28</td></tr><tr><td>mean</td><td>14.5</td><td>14.5</td><td>15.5</td><td>16.5</td><td>17.5</td><td>18.5</td></tr><tr><td>stddev</td><td>8.225975119502044</td><td>8.225975119502044</td><td>8.225975119502044</td><td>8.225975119502044</td><td>8.225975119502044</td><td>8.225975119502044</td></tr><tr><td>min</td><td>1</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td></tr><tr><td>max</td><td>28</td><td>28</td><td>29</td><td>30</td><td>31</td><td>32</td></tr></tbody></table></div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["We can do the same for one column."],"metadata":{}},{"cell_type":"code","source":["display(df.describe('Price'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o300.describe.\n: org.apache.spark.sql.AnalysisException: cannot resolve &#39;`Price`&#39; given input columns: [default.data1_csv.c3, default.data1_csv.c1, default.data1_csv.c4, default.data1_csv.c5, default.data1_csv.id, default.data1_csv.c2];;\n&#39;Project [&#39;Price]\n+- Project [id#1398, c1#1399, c2#1400, c3#1401, c4#1402, c5#1403]\n   +- SubqueryAlias `default`.`data1_csv`\n      +- Relation[id#1398,c1#1399,c2#1400,c3#1401,c4#1402,c5#1403] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:120)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:122)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$4.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:147)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:82)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:82)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:82)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:80)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3527)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1361)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1379)\n\tat org.apache.spark.sql.Dataset.describe(Dataset.scala:2545)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-514419398957166&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>display<span class=\"ansi-blue-fg\">(</span>df<span class=\"ansi-blue-fg\">.</span>describe<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;Price&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">describe</span><span class=\"ansi-blue-fg\">(self, *cols)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1198</span>         <span class=\"ansi-green-fg\">if</span> len<span class=\"ansi-blue-fg\">(</span>cols<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">==</span> <span class=\"ansi-cyan-fg\">1</span> <span class=\"ansi-green-fg\">and</span> isinstance<span class=\"ansi-blue-fg\">(</span>cols<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> list<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1199</span>             cols <span class=\"ansi-blue-fg\">=</span> cols<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-fg\">-&gt; 1200</span><span class=\"ansi-red-fg\">         </span>jdf <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>describe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jseq<span class=\"ansi-blue-fg\">(</span>cols<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1201</span>         <span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>jdf<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1202</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#34;cannot resolve &#39;`Price`&#39; given input columns: [default.data1_csv.c3, default.data1_csv.c1, default.data1_csv.c4, default.data1_csv.c5, default.data1_csv.id, default.data1_csv.c2];;\\n&#39;Project [&#39;Price]\\n+- Project [id#1398, c1#1399, c2#1400, c3#1401, c4#1402, c5#1403]\\n   +- SubqueryAlias `default`.`data1_csv`\\n      +- Relation[id#1398,c1#1399,c2#1400,c3#1401,c4#1402,c5#1403] csv\\n&#34;</div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["**Challenge #1**\n\n\nLooking at the count of values for the Price column, how many rows in our dataset our missing values for Price?"],"metadata":{}},{"cell_type":"markdown","source":["**Challenge #2**\n\nWhich two other columns appear to be missing data?"],"metadata":{}},{"cell_type":"markdown","source":["### Data preparation"],"metadata":{}},{"cell_type":"markdown","source":["When examining the summary stats, one problem may have jumped out at you in the Price column. The Max price is $9,995.00 but the Mean price is $10,728.00. This does not make sense (e.g., the max price should be equal to or greater than the mean). Let's explore the data a little more to find out why.\n\nRemember how all types are string. This is probably something we should fix. In fact, except for FuelType, all of the columns in this data should be numeric. \n\nRun the following cell to create a new DataFrame where all of the numeric cells are of the correct data type.\n\nBe sure to update the table name  \"usedcars\\_#####\" with the unique name created while running the <a href=\"$./02 Loading data with Azure Databricks\">Loading data with Azure Databricks</a> notebook."],"metadata":{}},{"cell_type":"code","source":["df_typed = spark.sql(\"SELECT cast(Price as int), cast(Age as int), cast(KM as int), FuelType, cast(HP as int), cast(MetColor as int), cast(Automatic as int), cast(CC as int), cast(Doors as int), cast(Weight as int) FROM usedcars_#####\")\ndf_typed"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o204.sql.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nmismatched input &#39;#&#39; expecting &lt;EOF&gt;(line 1, pos 207)\n\n== SQL ==\nSELECT cast(Price as int), cast(Age as int), cast(KM as int), FuelType, cast(HP as int), cast(MetColor as int), cast(Automatic as int), cast(CC as int), cast(Doors as int), cast(Weight as int) FROM usedcars_#####\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------^^^\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:241)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:117)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:53)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:69)\n\tat com.databricks.sql.parser.DatabricksSqlParser$$anonfun$parsePlan$1.apply(DatabricksSqlParser.scala:64)\n\tat com.databricks.sql.parser.DatabricksSqlParser$$anonfun$parsePlan$1.apply(DatabricksSqlParser.scala:61)\n\tat com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:84)\n\tat com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:61)\n\tat org.apache.spark.sql.SparkSession$$anonfun$6.apply(SparkSession.scala:694)\n\tat org.apache.spark.sql.SparkSession$$anonfun$6.apply(SparkSession.scala:694)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:693)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">ParseException</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-514419398957171&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>df_typed <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;SELECT cast(Price as int), cast(Age as int), cast(KM as int), FuelType, cast(HP as int), cast(MetColor as int), cast(Automatic as int), cast(CC as int), cast(Doors as int), cast(Weight as int) FROM usedcars_#####&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> df_typed\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">sql</span><span class=\"ansi-blue-fg\">(self, sqlQuery)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    834</span>         <span class=\"ansi-blue-fg\">[</span>Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row1&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row2&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row3&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    835</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 836</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jsparkSession<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span>sqlQuery<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_wrapped<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    837</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    838</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">2.0</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     72</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.parser.ParseException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 73</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> ParseException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     74</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.streaming.StreamingQueryException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     75</span>                 <span class=\"ansi-green-fg\">raise</span> StreamingQueryException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">ParseException</span>: &#34;\\nmismatched input &#39;#&#39; expecting &lt;EOF&gt;(line 1, pos 207)\\n\\n== SQL ==\\nSELECT cast(Price as int), cast(Age as int), cast(KM as int), FuelType, cast(HP as int), cast(MetColor as int), cast(Automatic as int), cast(CC as int), cast(Doors as int), cast(Weight as int) FROM usedcars_#####\\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------^^^\\n&#34;</div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["Now that we have fixed up the data types, let's revisit the statistical summary."],"metadata":{}},{"cell_type":"code","source":["display(df_typed.describe())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-514419398957173&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>display<span class=\"ansi-blue-fg\">(</span>df_typed<span class=\"ansi-blue-fg\">.</span>describe<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;df_typed&#39; is not defined</div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["As can be seen in the above output, now the Price summary makes sense because the values are being properly handled as integers instead of strings. The min price is $4,350, the mean price is $10,728 and the max price is $32,500."],"metadata":{}},{"cell_type":"markdown","source":["Now let's turn our attention to the FuelType and understand what values we have in that column:"],"metadata":{}},{"cell_type":"code","source":["display(df_typed.select(\"FuelType\").distinct())"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["As the above output shows, we have various issues with the FuelType values:\n- The values have inconsistent casing (e.g., Diesel and diesel)\n- We have three values that effectively mean the same thing (CNG, CompressedNaturalGas and methane).\n\nLet's cleanup these values in our DataFrame. We want to perform these transformations:\n- \"Diesel\" to \"diesel\"\n- \"Petrol\" to \"petrol\"\n- \"CompressedNaturalGas\" to \"cng\"\n- \"methane\" to \"cng\"\n- \"CNG\" to \"cng\"\n\nWe can use the replace() method of the na subpackage of the DataFrame to easily describe and apply our transformation in way that will work at scale."],"metadata":{}},{"cell_type":"code","source":["df_cleaned_fueltype = df_typed.na.replace([\"Diesel\",\"Petrol\",\"CompressedNaturalGas\",\"methane\",\"CNG\"],[\"diesel\",\"petrol\",\"cng\",\"cng\",\"cng\"],\"FuelType\")\ndisplay(df_cleaned_fueltype.select(\"FuelType\").distinct())"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["Now for the last bit of cleanup- let's address the rows that have missing (null) values. Recall from our previous exploration that the columns Price, Age and KM each had rows with missing values. \n\nYou typically handle missing values either by deleting the rows that have them or filling them in with a suitable computed valued (sometimes called data imputation). While how you handle missing values depends on the situation, in our case we just want to delete the rows that having missing values."],"metadata":{}},{"cell_type":"code","source":["df_cleaned_of_nulls = df_cleaned_fueltype.na.drop(\"any\",subset=[\"Price\", \"Age\", \"KM\"])\ndisplay(df_cleaned_of_nulls.describe())"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["**Challenge #3**\n\n\nAfter cleaning your dataset of rows having any missing values, how many rows does your data set have?"],"metadata":{}},{"cell_type":"markdown","source":["Next, we want to save this prepared dataset as a global table so that we could use the cleansed data easily such as for further data understanding efforts or for modeling, irrespective of which Databrick cluster we end up using later on.\n\nTo do so, execute the following cell. \n\nBe sure to update the table name  \"usedcars\\_clean\\_#####\" (replace ##### to make the name unique within your environment - we recommend using the same ##### value you used while running the <a href=\"$./02 Loading data with Azure Databricks\">Loading data with Azure Databricks</a> notebook)."],"metadata":{}},{"cell_type":"code","source":["df_cleaned_of_nulls.write.mode(\"overwrite\").saveAsTable(\"usedcars_clean_#####\")"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["### Correlation analysis"],"metadata":{}},{"cell_type":"markdown","source":["Finally, lets explore the relationship our data shows between the price of the car and the age of that car for cars that run on petrol only.\n\nRun the following cell. Observe that a Scatter Plot chart type was selected. If you examine the Plot Options, notice that we have charted Age against Price. \n\nBe sure to update the table name  \"usedcars\\_clean\\_#####\" with the unique name created previously in this notebook."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT Price, Age FROM usedcars_clean_##### WHERE FuelType = 'petrol'"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["** Challenge #4**\n\nCan you explain what the chart suggests about the data?"],"metadata":{}},{"cell_type":"markdown","source":["Achieve the same using the matplotlib and pandas style:"],"metadata":{}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nfig, ax = plt.subplots()\npdf = df_cleaned_of_nulls.toPandas()\nax.scatter(pdf.Age, pdf.Price)\n\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["Run the following two cells to see the distribution of KM and the relationship between KM and Price. Note the use of np.arrange to create the array used for bins in the histogram.\n\nDo you notice anything out of the ordinary?"],"metadata":{}},{"cell_type":"code","source":["fig, ax = plt.subplots()\n\nbins= np.arange(0, 250000, 5000)\npdf['KM'].plot(kind='hist')\n\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["fig, ax = plt.subplots()\nax.scatter(pdf.KM, pdf.Price)\ndisplay(fig)\n"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["Using Pandas it is very easy to calculate the correlations between all features:\n\n```python\ndataframe.corr()\n```\n\nWe only want the correlation for features that are not categorical (remember that we consider binary features as categorical).   \nIn our dataset this corresponds to the features Age, KM, Weight, CC and HP.\n\n__Exercise:__ Calculate the correlation matrix for all features that are not categorical. Remember to include `Price` since we also want the correlations between the features and the sales price."],"metadata":{}},{"cell_type":"code","source":["# Run this cell and a very nice matrix will hopefully appear\nfig, ax = plt.subplots()\nsns.heatmap(pdf[['Price','Age', 'KM', 'Weight', 'CC', 'HP']].corr(),annot=True, center=0, cmap='BrBG', annot_kws={\"size\": 14})\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["Even with our limited knowledge about cars we expected a stronger correlation between horsepower and weight, and also between horsepower and displacement.  \nHowever, we also know that diesel-engines are very different from petrol-engines, and so mixing these two types can make the correlation very weak. \n\n__Exercise__: Plot the correlation matrix for all cars using petrol as fuel."],"metadata":{}},{"cell_type":"code","source":["### Your code goes here"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["__Exercise__: Plot the correlation matrix for all cars using diesel"],"metadata":{}},{"cell_type":"code","source":["### Your code goes here"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["Look at that! We got a reasonable correlation between HP and CC and between HP and Weight.   \nWe can also see that the correlation between HP and Price,  Weight and Price and KM and Price increased when we split the data on the fueltypes petrol and diesel.  \n\n__This is very interesting and worth a closer look! (left as an exercise)__\n\nYou are now ready to move to the next step: <a href=\"$./04 Advanced EDA with Azure Databricks\">Advanced EDA with Azure Databricks</a>"],"metadata":{}},{"cell_type":"markdown","source":["# Answers to Challenges\n1. Seven rows are missing Price data. There are 1446 rows in the data set, but only 1439 of them have a value for Price.\n2. Age and KM also have fewer that 1446 values. \n3. There should be 1436 rows after removing rows with missing values.\n4. The chart suggests that the Price of the car appears to go down with an increase in Age. So the older the car, the cheaper it is."],"metadata":{}}],"metadata":{"name":"03 Basic EDA with Azure Databricks","notebookId":514419398957147},"nbformat":4,"nbformat_minor":0}
